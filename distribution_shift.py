import numpy as np
import sys
import argparse
from tqdm import tqdm
from cache.LRU_v2 import LRUCache
from cache.two_q import TwoQCache
from cache.ARC import ARCCache
from cache.ARC_PQ import ARCCachePQ
from cache.DBL import DBLCache
from cache.DBL_PQ import DBLCachePQ
from cache.LFU import LFUCache

def read_block_data_v3(path):
    with open(path, "r") as f:
        lines = [line.strip() for line in f.readlines()]
    
    data = [[(int(num), str(i + 1)) for num in line.split()]  # ÊØèË°å‰Ωú‰∏∫‰∏Ä‰∏™Â≠êÂàóË°®
            for i, line in enumerate(lines) if line]  # ËøáÊª§Á©∫Ë°å
    # ÁªüËÆ°ÈïøÂ∫¶
    # lengths = [len(block) for block in data]
    # avg_len = sum(lengths) / len(lengths)
    # print(f"üìä Total blocks: {len(data)}")
    # print(f"üìè Average block length: {avg_len:.2f}")
    # print(f"üî¢ Min: {min(lengths)}, Max: {max(lengths)}")
    
    return data

def power_law_sampling(num_elements, sequence_length=1500, exponent=1.0):
    values = np.arange(1, num_elements + 1)
    probabilities = values ** -exponent
    probabilities /= probabilities.sum()
    sampled_indices = np.random.choice(values - 1, size=sequence_length, p=probabilities)
    return [data[i] for i in sampled_indices]

def power_law_with_hotspot(data, total_length=1500, exponent=1.0, 
                           window_size=50, hotspot_ratio=0.1, hotspot_boost=10):
    num_windows = total_length // window_size
    result = []
    num_elements = len(data)
    values = np.arange(1, num_elements + 1)
    base_prob = values ** -exponent
    base_prob /= base_prob.sum()

    for _ in range(num_windows):
        hotspot_indices = np.random.choice(num_elements, size=max(1, int(hotspot_ratio * window_size)), replace=False)
        prob = base_prob.copy()
        prob[hotspot_indices] *= hotspot_boost
        prob /= prob.sum()
        
        sampled_indices = np.random.choice(num_elements, size=window_size, p=prob)
        # print('hotspot_indices', hotspot_indices)
        # print('sampled_indices', sampled_indices)
        result.extend([data[i] for i in sampled_indices])

    return result

def pure_hotspot_sampling(data, sequence_length=1500, 
                          hotspot_fraction=0.1, hotspot_access_ratio=0.8):
    """
    - hotspot_fraction: ÁÉ≠ÁÇπÊñáÊ°£Âç†ÂÖ®ÈÉ®ÊñáÊ°£ÁöÑÊØî‰æã
    - hotspot_access_ratio: ÊúÄÁªàËÆøÈóÆÂ∫èÂàó‰∏≠ÔºåÁÉ≠ÁÇπÊñáÊ°£Âç†Â§öÂ∞ëÊØî‰æã
    """
    num_elements = len(data)
    num_hotspots = max(1, int(hotspot_fraction * num_elements))

    # ÈöèÊú∫ÈÄâÊã©Âõ∫ÂÆöÁöÑÁÉ≠ÁÇπÊñáÊ°£
    hotspot_indices = np.random.choice(num_elements, size=num_hotspots, replace=False)
    all_indices = np.arange(num_elements)
    cold_indices = np.setdiff1d(all_indices, hotspot_indices)

    num_hotspot_accesses = int(sequence_length * hotspot_access_ratio)
    num_cold_accesses = sequence_length - num_hotspot_accesses

    # ÈöèÊú∫ËÆøÈóÆ
    hotspot_samples = np.random.choice(hotspot_indices, size=num_hotspot_accesses, replace=True)
    cold_samples = np.random.choice(cold_indices, size=num_cold_accesses, replace=True)

    # Ê∑∑ÂêàÊâì‰π±È°∫Â∫è
    all_samples = np.concatenate([hotspot_samples, cold_samples])
    np.random.shuffle(all_samples)

    return [data[i] for i in all_samples], all_samples.tolist()

def windowed_powerlaw_sampling(data, total_length=6000, window_size=6000, alpha=1.0, shuffle_each_window=True):
    num_windows = total_length // window_size
    result = []

    for _ in range(num_windows):
        if shuffle_each_window:
            np.random.shuffle(data)
        
        num_elements = len(data)
        values = np.arange(1, num_elements + 1)
        base_prob = values ** -alpha
        base_prob /= base_prob.sum()

        sampled_indices = np.random.choice(num_elements, size=window_size, p=base_prob)
        result.extend([data[i] for i in sampled_indices])

    return result



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test LRUCache and TwoQCache")
    parser.add_argument("--alpha", type=float, default=1.0, help="Exponent for power law sampling")
    parser.add_argument("--cache_size_fraction", type=float, default=0.1, help="Fraction of cache occupied by one data entry")
    parser.add_argument("--sequence_length", type=float, default=750, help="Numbers of prompts")
    args = parser.parse_args()

    alpha = float(args.alpha)
    cache_size_fraction = float(args.cache_size_fraction)
    sequence_length = int(args.sequence_length)
    
    np.random.seed(42)
    data_path = "/Users/shenyang/Desktop/MS Research/workplace/data/142_docs.txt"
    data_path = "/Users/shenyang/Desktop/MS Research/workplace/data/artificial_docs.txt"
    max_size = int(668 / cache_size_fraction)
    k_value = int(max_size * 0.5)
    
    data = read_block_data_v3(data_path)[:]
    # selected_inputs = power_law_sampling(len(data),sequence_length=sequence_length, exponent=alpha)
    # selected_inputs = power_law_with_hotspot(
    #     data, total_length=sequence_length, exponent=alpha,
    #     window_size=20, hotspot_ratio=0.1, hotspot_boost=10     # 50 0.1 10
    # )
    # selected_inputs, selected_indices = pure_hotspot_sampling(
    #     data=data, 
    #     sequence_length=sequence_length, 
    #     hotspot_fraction=0.1,          # 5 ‰∏™ÁÉ≠ÁÇπÊñáÊ°£
    #     hotspot_access_ratio=0.8       # ÁÉ≠ÁÇπËÆøÈóÆÂç† 80%
    # )
    selected_inputs = windowed_powerlaw_sampling(
        data,
        alpha=alpha
    )


    data = selected_inputs

    lru_cache = LRUCache(max_size=max_size)
    for row in data:
        for key, value in row:
            lru_cache.get(key)
        for key, value in reversed(row):
            lru_cache.put(key, value)
    print(f"LRUCache Hit Rate: {lru_cache.hit_rate():.2%}")
    
    dbl_cache = DBLCachePQ(max_size=max_size)
    for idx, row in enumerate(data):
        for key, value in row:
            dbl_cache.get(key)
        for key, value in reversed(row):
            dbl_cache.put(key, value)
        # print(f"Step {idx} DBLCache A1: {len(dbl_cache.A1in_data)}, Am: {len(dbl_cache.Am_data)}")
    print(f"DBLCache Hit Rate: {dbl_cache.hit_rate():.2%}")
    
    lfu_cache = LFUCache(max_size=max_size)
    for idx, row in enumerate(data):
        for key, value in row:
            lfu_cache.get(key)
        for key, value in reversed(row):
            lfu_cache.put(key, value)
        # print(f"Step {idx} DBLCache A1: {len(lfu_cache.A1in_data)}, Am: {len(lfu_cache.Am_data)}")
        # print(f"Step {idx} lfu_cache.min_freq {lfu_cache.min_freq}")
    print(f"LFUCache Hit Rate: {lfu_cache.hit_rate():.2%}")
    
    # dbl_cache_pq = DBLCachePQ(max_size=max_size)
    # for row in data:
    #     for key, value in row:
    #         dbl_cache_pq.get(key)
    #     for key, value in reversed(row):
    #         dbl_cache_pq.put(key, value)
    # print(f"DBLCachePQ Hit Rate: {dbl_cache_pq.hit_rate():.2%}")

    # two_q_cache = TwoQCache(max_size=max_size, k=k_value)
    # for row in data:
    #     for key, value in row:
    #         two_q_cache.get(key)
    #     for key, value in reversed(row):
    #         two_q_cache.put(key, value)
    # print(f"TwoQCache Hit Rate: {two_q_cache.hit_rate():.2%}")
    
    arc_cache = ARCCache(max_size=max_size)
    for idx, row in enumerate(tqdm(data)):
        for key, value in row:
            arc_cache.get(key)
        for key, value in reversed(row):
            arc_cache.put(key, value)
    print(f"ARCCache Hit Rate: {arc_cache.hit_rate():.2%}")
    
    # arc_pq_cache = ARCCachePQ(max_size=max_size)
    # for idx, row in enumerate(tqdm(data)):
    #     for key, value in row:
    #         arc_pq_cache.get(key)
    #     for key, value in reversed(row):
    #         arc_pq_cache.put(key, value)
    # print(f"ARCCache(PQ) Hit Rate: {arc_pq_cache.hit_rate():.2%}")

    # result_filename = f"./result/full_results_alpha_{alpha}.txt"
    # with open(result_filename, "a") as f:
    #     f.write(f"{cache_size_fraction},{lru_cache.hit_rate():.4f},{two_q_cache.hit_rate():.4f},{arc_cache.hit_rate():.4f}\n")